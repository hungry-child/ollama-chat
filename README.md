# ollama-chat
chat with ollama llm models running locally
